<div align="center">

# Baichuan-M2-32B

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Model-yellow)](https://huggingface.co/baichuan-inc/Baichuan-M2-32B)
[![M2 GPTQ-4bit](https://img.shields.io/badge/ğŸ¤—%20M2%20GPTQ--4bit-Model-orange)](https://huggingface.co/baichuan-inc/Baichuan-M2-32B-GPTQ-Int4)
[![åä¸ºæ˜‡è…¾ 8bit](https://img.shields.io/badge/âœ¨%20åä¸ºæ˜‡è…¾%208bit-Model-green)](https://modelers.cn/models/Baichuan/Baichuan-M2-32B-W8A8)

<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/baichuan-inc/Baichuan-M2-32B/blob/main/README_en.md">English</a>
    <p>
</h4>

</div>

## ğŸŒŸ æ¨¡å‹ç®€ä»‹

Baichuan-M2-32B æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„åŒ»ç–—å¢å¼ºæ¨ç†æ¨¡å‹ï¼Œè¿™æ˜¯ç™¾å·å¼€æºå‘å¸ƒçš„ç¬¬äºŒä¸ªåŒ»ç–—å¢å¼ºæ¨¡å‹ï¼Œä¸“ä¸ºçœŸå®ä¸–ç•Œçš„åŒ»ç–—æ¨ç†ä»»åŠ¡è®¾è®¡ã€‚è¯¥æ¨¡å‹åŸºäº Qwen2.5-32B åŸºåº§ï¼Œé€šè¿‡åˆ›æ–°çš„å¤§å‹éªŒè¯å™¨ç³»ç»Ÿï¼ˆLarge Verifier Systemï¼‰ä»çœŸå®ä¸–ç•Œçš„åŒ»ç–—é—®é¢˜å‡ºå‘ï¼Œè¿›è¡ŒåŒ»ç–—é¢†åŸŸåè®­ç»ƒå¯¹é½ï¼Œåœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†åŒ»ç–—æ•ˆæœçš„çªç ´æ€§æå‡ã€‚

**æ¨¡å‹ç‰¹ç‚¹ï¼š**

Baichuan-M2 é‡‡ç”¨äº†ä¸‰ä¸ªæ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ï¼šé¦–å…ˆé€šè¿‡**å¤§å‹éªŒè¯å™¨ç³»ç»Ÿ**ï¼Œç»“åˆåŒ»ç–—åœºæ™¯ç‰¹ç‚¹è®¾è®¡äº†å…¨é¢çš„åŒ»ç–—éªŒè¯ä½“ç³»ï¼ŒåŒ…å«æ‚£è€…æ¨¡æ‹Ÿå™¨å’Œå¤šç»´åº¦éªŒè¯æœºåˆ¶ï¼›å…¶æ¬¡é€šè¿‡**åŒ»ç–—é¢†åŸŸé€‚åº”æ€§å¢å¼º**çš„ä¸­æœŸè®­ç»ƒï¼ˆMid-Trainingï¼‰ï¼Œåœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶å®ç°è½»é‡é«˜æ•ˆçš„åŒ»ç–—é¢†åŸŸé€‚åº”ï¼›æœ€åé‡‡ç”¨**å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ **ç­–ç•¥ï¼Œå°†å¤æ‚çš„ RL ä»»åŠ¡åˆ†è§£ä¸ºå±‚æ¬¡åŒ–çš„è®­ç»ƒé˜¶æ®µï¼Œé€æ­¥æå‡æ¨¡å‹çš„åŒ»å­¦å¸¸è¯†ã€æ¨ç†å’Œæ‚£è€…äº¤äº’èƒ½åŠ›ã€‚

**æ ¸å¿ƒäº®ç‚¹ï¼š**
- ğŸ† **å…¨çƒæœ€å¼ºåŒ»ç–—å¼€æºæ¨¡å‹**ï¼šåœ¨ HealthBench è¯„æµ‹é›†ä¸Šè¶…è¶Šæ‰€æœ‰å¼€æºæ¨¡å‹åŠä¼—å¤šå‰æ²¿é—­æºæ¨¡å‹ï¼Œæ˜¯æœ€æ¥è¿‘ GPT-5 åŒ»ç–—èƒ½åŠ›çš„å¼€æºå¤§æ¨¡å‹
- ğŸ§  **åŒ»ç”Ÿæ€ç»´å¯¹é½**ï¼šåŸºäºçœŸå®ç—…ä¾‹æ•°æ®å’Œæ‚£è€…æ¨¡æ‹Ÿå™¨è®­ç»ƒï¼Œå…·å¤‡ä¸´åºŠè¯Šæ–­æ€ç»´å’Œé²æ£’çš„åŒ»æ‚£äº¤äº’èƒ½åŠ›
- âš¡ **é«˜æ•ˆéƒ¨ç½²ä¸æ¨ç†**ï¼šæ”¯æŒ 4bit é‡åŒ–åœ¨ RTX4090 å•å¡éƒ¨ç½²ï¼ŒMTP ç‰ˆæœ¬å•ç”¨æˆ·åœºæ™¯ä¸‹ token ååæå‡ 58.5%



## ğŸ“Š æ€§èƒ½è¡¨ç°

### HealthBenchæŒ‡æ ‡

| æ¨¡å‹åç§° | HealthBench | HealthBench-Hard | HealthBench-Consensus |
|----------|-------------|------------------|-----------------------|
| Baichuan-M2 | 60.1 | 34.7 | 91.5 |
| gpt-oss-120b | 57.6 | 30 | 90 |
| Qwen3-235B-A22B-Thinking-2507 | 55.2 | 25.9 | 90.6 |
| Deepseek-R1-0528 | 53.6 | 22.6 | 91.5 |
| GLM-4.5 | 47.8 | 18.7 | 85.3 |
| Kimi-K2 | 43 | 10.7 | 90.9 |
| gpt-oss-20b | 42.5 | 10.8 | 82.6 |

### é€šç”¨æŒ‡æ ‡

| è¯„æµ‹é›† | Baichuan-M2-32B | Qwen3-32B (thinking) |
|--------|-----------------|-----------|
| AIME24 | 83.4 | 81.4 |
| AIME25 | 72.9 | 72.9 |
| Arena-Hard-v2.0 | 45.8 | 44.5 |
| CFBench | 77.6 | 75.7 |
| WritingBench | 8.56 | 7.90 |

*å¤‡æ³¨ï¼šAIME çš„ max_tokens è®¾ä¸º 64kï¼Œå…¶ä»–è¯„æµ‹é›†è®¾ä¸º 32kï¼Œtemperature ç»Ÿä¸€ä¸º 0.6ã€‚*


## ğŸ› ï¸ æŠ€æœ¯ç‰¹è‰²

ğŸ“— **æŠ€æœ¯åšå®¢**ï¼š[Blog - Baichuan-M2](https://www.baichuan-ai.com/blog/baichuan-M2)

### å¤§å‹éªŒè¯å™¨ç³»ç»Ÿ
- **æ‚£è€…æ¨¡æ‹Ÿå™¨**ï¼šåŸºäºçœŸå®ç—…ä¾‹æ„å»ºçš„è™šæ‹Ÿæ‚£è€…ç³»ç»Ÿ
- **å¤šç»´åº¦éªŒè¯**ï¼šåŒ»å­¦å‡†ç¡®æ€§ã€å›ç­”å®Œæ•´æ€§ã€è¿½é—®æ„ŸçŸ¥ç­‰ 8 ä¸ªç»´åº¦
- **åŠ¨æ€è¯„åˆ†**ï¼šå®æ—¶ç”Ÿæˆè¯„åˆ†æ ‡å‡†ï¼Œé€‚åº”å¤æ‚ä¸´åºŠç¯å¢ƒ

### åŒ»ç–—é¢†åŸŸé€‚åº”
- **Mid-Training**ï¼šåŒ»ç–—çŸ¥è¯†æ³¨å…¥çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›
- **å¼ºåŒ–å­¦ä¹ **ï¼šå¤šé˜¶æ®µ RL ç­–ç•¥ä¼˜åŒ–
- **é€šä¸“å…¼é¡¾**ï¼šç²¾å¿ƒé…æ¯”çš„åŒ»ç–—ã€é€šç”¨ä¸æ•°å­¦é¢†åŸŸå¤åˆæ•°æ®

<div align="center">
  <img src="images/m2-framework.png" alt="Beyond the Model: Scaling Medical Capability with a Large Verifier System" width="80%">
</div>

## ğŸ”§ å¿«é€Ÿå¼€å§‹

```python
# 1. load model
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-M2-32B", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-M2-32B")
# 2. Input prompt text
prompt = "Got a big swelling after a bug bite. Need help reducing it."
# 3. Encode the input text for the model
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    thinking_mode='on' # on/off/auto
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
# 4. Generate text
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=4096
)
output_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
][0]
# 5. parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)
print("content:", content)

```

## âš ï¸ ä½¿ç”¨é¡»çŸ¥

1. **åŒ»ç–—å…è´£å£°æ˜**ï¼šæœ¬æ¨¡å‹ä»…ä¾›ç ”ç©¶å’Œå‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å»ºè®®
2. **é€‚ç”¨åœºæ™¯**ï¼šåŒ»å­¦æ•™è‚²ã€å¥åº·å’¨è¯¢ã€ä¸´åºŠè¾…åŠ©å†³ç­–ç­‰
3. **å®‰å…¨ä½¿ç”¨**ï¼šå»ºè®®åœ¨ä¸“ä¸šåŒ»ç–—äººå‘˜æŒ‡å¯¼ä¸‹ä½¿ç”¨

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [Apache License 2.0](LICENSE) å¼€æºåè®®ï¼Œæ¬¢è¿ç ”ç©¶å’Œå•†ä¸šä½¿ç”¨ã€‚

## ğŸ¤ è‡´è°¢

- åŸºç¡€æ¨¡å‹ï¼šQwen2.5-32B
- è®­ç»ƒæ¡†æ¶ï¼šverl
- æ¨ç†å¼•æ“ï¼švLLMã€SGLang
- é‡åŒ–æ–¹æ³•ï¼šAutoRoundã€GPTQ

æ„Ÿè°¢å¼€æºç¤¾åŒºçš„è´¡çŒ®ï¼Œæˆ‘ä»¬å°†æŒç»­å›é¦ˆç¤¾åŒºï¼Œæ¨åŠ¨åŒ»ç–— AI æŠ€æœ¯å‘å±•ã€‚

## ğŸ“ è”ç³»æˆ‘ä»¬

- æ›´å¤šèµ„æºï¼š[ç™¾å·æ™ºèƒ½å®˜ç½‘](https://www.baichuan-ai.com)

- æŠ€æœ¯äº¤æµï¼š[GitHub](https://github.com/baichuan-inc)

---

<div align="center">

**è®©AIåŠ©åŠ›åŒ»ç–—ï¼Œè®©å¥åº·è§¦æ‰‹å¯åŠ**

</div>